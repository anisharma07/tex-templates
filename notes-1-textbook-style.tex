\documentclass[11pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=black]{hyperref}
\usepackage{enumitem}
\usepackage{tcolorbox}
% Custom title page formatting
\makeatletter
\renewcommand\tableofcontents{%
    \begin{center}
    {\Large\bfseries\contentsname}\par
    \vspace{1em}
    \end{center}
    \@starttoc{toc}
}
\makeatother

% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{property}[theorem]{Property}

\title{
    \vspace{2cm}
    {\Huge\bfseries Chapter 3}
    \vspace{1cm}
    
    {\huge Growth of Functions}
    \vspace{1cm}
    
    {\Large Asymptotic Analysis of Algorithms}
    \vspace{2cm}
}
\author{\Large Computer Science Study Notes}
\date{\today}

\begin{document}

\begin{titlepage}
    \centering
    \vfill
    {\Huge\bfseries Chapter 3}\par
    \vspace{1.5cm}
    {\huge Growth of Functions}\par
    \vspace{1cm}
    {\Large Asymptotic Analysis of Algorithms}\par
    \vfill
    {\Large Computer Science Study Notes}\par
    \vspace{0.5cm}
    {\large \today}\par
    \vfill
\end{titlepage}
\newpage

\tableofcontents
\newpage

\section{Introduction}

The order of growth of an algorithm's running time provides a simple yet powerful characterization of the algorithm's efficiency. It also enables us to compare the relative performance of alternative algorithms. Consider the comparison between merge sort and insertion sort: once the input size $n$ becomes sufficiently large, merge sort with its $\Theta(n \lg n)$ worst-case running time outperforms insertion sort, which has a worst-case running time of $\Theta(n^2)$.

Although we can sometimes determine the exact running time of an algorithm (as was done for insertion sort in Chapter 2), the additional precision is often not worth the computational effort required. For sufficiently large inputs, the multiplicative constants and lower-order terms of an exact running time are dominated by the effects of the input size itself.

\subsection{Asymptotic Efficiency}

When we examine input sizes large enough to make only the order of growth of the running time relevant, we are studying the \textbf{asymptotic efficiency} of algorithms. That is, we are concerned with how the running time of an algorithm increases with the size of the input in the limit, as the size increases without bound. Usually, an algorithm that is asymptotically more efficient will be the best choice for all but very small inputs.

This chapter provides several standard methods for simplifying asymptotic analysis. We begin by defining several types of asymptotic notation, then present notational conventions used throughout algorithm analysis, and finally review the behavior of functions that commonly arise in the analysis of algorithms.

\section{Asymptotic Notation}

The notations we use to describe the asymptotic running time of an algorithm are defined in terms of functions whose domains are the set of natural numbers $\mathbb{N} = \{0, 1, 2, \ldots\}$. Such notations are convenient for describing the worst-case running-time function $T(n)$, which is usually defined only on integer input sizes.

\subsection{Asymptotic Notation, Functions, and Running Times}

We use asymptotic notation primarily to describe the running times of algorithms. However, asymptotic notation actually applies to functions in general. When we apply asymptotic notation to characterize algorithm running times, we typically focus on one of several cases:
\begin{itemize}
\item \textbf{Worst-case running time:} The maximum time over all inputs of size $n$
\item \textbf{Best-case running time:} The minimum time over all inputs of size $n$
\item \textbf{Average-case running time:} The expected time over all inputs of size $n$
\end{itemize}

\subsection{$\Theta$-Notation}

In Chapter 2, we found that the worst-case running time of insertion sort is $T(n) = \Theta(n^2)$. Let us now define precisely what this notation means.

\begin{definition}[$\Theta$-notation]
For a given function $g(n)$, we denote by $\Theta(g(n))$ the set of functions:
\begin{align*}
\Theta(g(n)) = \{f(n) : &\text{ there exist positive constants } c_1, c_2, \text{ and } n_0 \text{ such that} \\
&0 \leq c_1 g(n) \leq f(n) \leq c_2 g(n) \text{ for all } n \geq n_0\}
\end{align*}
\end{definition}

A function $f(n)$ belongs to the set $\Theta(g(n))$ if there exist positive constants $c_1$ and $c_2$ such that $f(n)$ can be ``sandwiched'' between $c_1 g(n)$ and $c_2 g(n)$ for sufficiently large $n$. Because $\Theta(g(n))$ is a set, we could write ``$f(n) \in \Theta(g(n))$'' to indicate that $f(n)$ is a member of $\Theta(g(n))$. Instead, we conventionally write ``$f(n) = \Theta(g(n))$'' to express the same notion, despite this being an abuse of equality notation.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{theta.png}
\caption{Graphical representation of $\Theta$-notation. For all values of $n$ at and to the right of $n_0$, the value of $f(n)$ lies at or above $c_1 g(n)$ and at or below $c_2 g(n)$. We say that $g(n)$ is an asymptotically tight bound for $f(n)$.}
\end{figure}

The definition of $\Theta(g(n))$ requires that every member $f(n) \in \Theta(g(n))$ be asymptotically nonnegative; that is, $f(n)$ must be nonnegative whenever $n$ is sufficiently large. Consequently, the function $g(n)$ itself must be asymptotically nonnegative, or else the set $\Theta(g(n))$ is empty. We shall assume that every function used within asymptotic notation is asymptotically nonnegative.

\begin{example}
We can use the formal definition to verify that $\frac{1}{2}n^2 - 3n = \Theta(n^2)$. To do so, we must determine positive constants $c_1, c_2$, and $n_0$ such that
$$c_1 n^2 \leq \frac{1}{2}n^2 - 3n \leq c_2 n^2$$
for all $n \geq n_0$. Dividing by $n^2$ yields
$$c_1 \leq \frac{1}{2} - \frac{3}{n} \leq c_2$$

We can make the right-hand inequality hold for any value of $n \geq 1$ by choosing any constant $c_2 \geq 1/2$. Likewise, we can make the left-hand inequality hold for any value of $n \geq 7$ by choosing any constant $c_1 \leq 1/14$. Thus, by choosing $c_1 = 1/14$, $c_2 = 1/2$, and $n_0 = 7$, we can verify that $\frac{1}{2}n^2 - 3n = \Theta(n^2)$.
\end{example}

\begin{theorem}[Polynomial Asymptotic Bounds]
For any polynomial $p(n) = \sum_{i=0}^d a_i n^i$ where the $a_i$ are constants and $a_d > 0$, we have $p(n) = \Theta(n^d)$.
\end{theorem}

\begin{proof}
The highest-order term $a_d n^d$ dominates all lower-order terms for sufficiently large $n$. Choose constants $c_1$ and $c_2$ such that the inequalities in the definition of $\Theta$-notation are satisfied.
\end{proof}

\subsection{$O$-Notation}

The $\Theta$-notation asymptotically bounds a function from above and below. When we have only an asymptotic upper bound, we use $O$-notation.

\begin{definition}[$O$-notation]
For a given function $g(n)$, we denote by $O(g(n))$ the set of functions:
$$O(g(n)) = \{f(n) : \text{ there exist positive constants } c \text{ and } n_0 \text{ such that } 0 \leq f(n) \leq cg(n) \text{ for all } n \geq n_0\}$$
\end{definition}

We use $O$-notation to give an upper bound on a function, to within a constant factor. Note that $f(n) = \Theta(g(n))$ implies $f(n) = O(g(n))$, since $\Theta$-notation is a stronger notion than $O$-notation. Written set-theoretically, we have $\Theta(g(n)) \subseteq O(g(n))$.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{O.png}
\caption{Graphical representation of $O$-notation. For all values $n$ at and to the right of $n_0$, the value of $f(n)$ is on or below $cg(n)$.}
\end{figure}

When we write $f(n) = O(g(n))$, we are claiming that some constant multiple of $g(n)$ is an asymptotic upper bound on $f(n)$, with no claim about how tight the bound is. For example, when $a > 0$, any linear function $an + b$ is in $O(n^2)$, which is easily verified by taking $c = a + |b|$ and $n_0 = \max(1, -b/a)$.

Using $O$-notation, we can often describe the running time of an algorithm merely by inspecting the algorithm's overall structure. For example, the doubly nested loop structure of insertion sort immediately yields an $O(n^2)$ upper bound on the worst-case running time.

\subsection{$\Omega$-Notation}

Just as $O$-notation provides an asymptotic upper bound on a function, $\Omega$-notation provides an asymptotic lower bound.

\begin{definition}[$\Omega$-notation]
For a given function $g(n)$, we denote by $\Omega(g(n))$ the set of functions:
$$\Omega(g(n)) = \{f(n) : \text{ there exist positive constants } c \text{ and } n_0 \text{ such that } 0 \leq cg(n) \leq f(n) \text{ for all } n \geq n_0\}$$
\end{definition}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{Omega.png}
\caption{Graphical representation of $\Omega$-notation. For all values $n$ at or to the right of $n_0$, the value of $f(n)$ is on or above $cg(n)$.}
\end{figure}

\begin{theorem}
For any two functions $f(n)$ and $g(n)$, we have $f(n) = \Theta(g(n))$ if and only if $f(n) = O(g(n))$ and $f(n) = \Omega(g(n))$.
\end{theorem}

As an example of the application of this theorem, the running time of insertion sort belongs to both $\Omega(n)$ and $O(n^2)$, since it falls anywhere between a linear function of $n$ and a quadratic function of $n$. Moreover, these bounds are asymptotically as tight as possible.

\subsection{$o$-Notation}

The asymptotic upper bound provided by $O$-notation may or may not be asymptotically tight. We use $o$-notation to denote an upper bound that is not asymptotically tight.

\begin{definition}[$o$-notation]
We formally define $o(g(n))$ as the set:
$$o(g(n)) = \{f(n) : \text{ for any positive constant } c > 0, \text{ there exists a constant } n_0 > 0 \text{ such that } 0 \leq f(n) < cg(n) \text{ for all } n \geq n_0\}$$
\end{definition}

For example, $2n = o(n^2)$, but $2n^2 \neq o(n^2)$. The main difference between $O$-notation and $o$-notation is that in $f(n) = O(g(n))$, the bound $0 \leq f(n) \leq cg(n)$ holds for some constant $c > 0$, but in $f(n) = o(g(n))$, the bound $0 \leq f(n) < cg(n)$ holds for all constants $c > 0$.

Intuitively, in $o$-notation, the function $f(n)$ becomes insignificant relative to $g(n)$ as $n$ approaches infinity:
$$\lim_{n \to \infty} \frac{f(n)}{g(n)} = 0$$

\subsection{$\omega$-Notation}

By analogy, $\omega$-notation is to $\Omega$-notation as $o$-notation is to $O$-notation.

\begin{definition}[$\omega$-notation]
We formally define $\omega(g(n))$ as the set:
$$\omega(g(n)) = \{f(n) : \text{ for any positive constant } c > 0, \text{ there exists a constant } n_0 > 0 \text{ such that } 0 \leq cg(n) < f(n) \text{ for all } n \geq n_0\}$$
\end{definition}

For example, $n^2/2 = \omega(n)$, but $n^2/2 \neq \omega(n^2)$. The relation $f(n) = \omega(g(n))$ implies that
$$\lim_{n \to \infty} \frac{f(n)}{g(n)} = \infty$$
if the limit exists. That is, $f(n)$ becomes arbitrarily large relative to $g(n)$ as $n$ approaches infinity.

\subsection{Comparing Functions}

Many of the relational properties of real numbers apply to asymptotic comparisons as well.

\begin{property}[Transitivity]
\begin{align*}
f(n) = \Theta(g(n)) \text{ and } g(n) = \Theta(h(n)) &\implies f(n) = \Theta(h(n)) \\
f(n) = O(g(n)) \text{ and } g(n) = O(h(n)) &\implies f(n) = O(h(n)) \\
f(n) = \Omega(g(n)) \text{ and } g(n) = \Omega(h(n)) &\implies f(n) = \Omega(h(n)) \\
f(n) = o(g(n)) \text{ and } g(n) = o(h(n)) &\implies f(n) = o(h(n)) \\
f(n) = \omega(g(n)) \text{ and } g(n) = \omega(h(n)) &\implies f(n) = \omega(h(n))
\end{align*}
\end{property}

\begin{property}[Reflexivity]
\begin{align*}
f(n) &= \Theta(f(n)) \\
f(n) &= O(f(n)) \\
f(n) &= \Omega(f(n))
\end{align*}
\end{property}

\begin{property}[Symmetry]
$$f(n) = \Theta(g(n)) \text{ if and only if } g(n) = \Theta(f(n))$$
\end{property}

\begin{property}[Transpose Symmetry]
\begin{align*}
f(n) = O(g(n)) &\text{ if and only if } g(n) = \Omega(f(n)) \\
f(n) = o(g(n)) &\text{ if and only if } g(n) = \omega(f(n))
\end{align*}
\end{property}

We can draw an analogy between the asymptotic comparison of two functions $f$ and $g$ and the comparison of two real numbers $a$ and $b$:
\begin{center}
\begin{tabular}{c@{\quad}c@{\quad}c}
$f(n) = O(g(n))$ & is like & $a \leq b$ \\
$f(n) = \Omega(g(n))$ & is like & $a \geq b$ \\
$f(n) = \Theta(g(n))$ & is like & $a = b$ \\
$f(n) = o(g(n))$ & is like & $a < b$ \\
$f(n) = \omega(g(n))$ & is like & $a > b$
\end{tabular}
\end{center}

However, not all functions are asymptotically comparable. For example, we cannot compare the functions $n$ and $n^{1+\sin n}$ using asymptotic notation, since the exponent oscillates between 0 and 2.

\section{Standard Notations and Common Functions}

This section reviews standard mathematical functions and notations and explores the relationships among them.

\subsection{Monotonicity}

\begin{definition}
A function $f(n)$ is:
\begin{itemize}
\item \textbf{Monotonically increasing} if $m \leq n$ implies $f(m) \leq f(n)$
\item \textbf{Monotonically decreasing} if $m \leq n$ implies $f(m) \geq f(n)$
\item \textbf{Strictly increasing} if $m < n$ implies $f(m) < f(n)$
\item \textbf{Strictly decreasing} if $m < n$ implies $f(m) > f(n)$
\end{itemize}
\end{definition}

\subsection{Floors and Ceilings}

For any real number $x$, we denote:
\begin{itemize}
\item $\lfloor x \rfloor$ = the greatest integer less than or equal to $x$ (floor)
\item $\lceil x \rceil$ = the least integer greater than or equal to $x$ (ceiling)
\end{itemize}

For all real $x$: $x - 1 < \lfloor x \rfloor \leq x \leq \lceil x \rceil < x + 1$

For any integer $n$: $\lceil n/2 \rceil + \lfloor n/2 \rfloor = n$

\subsection{Modular Arithmetic}

For any integer $a$ and any positive integer $n$, the value $a \bmod n$ is the remainder of the quotient $a/n$:
$$a \bmod n = a - n\lfloor a/n \rfloor$$

It follows that $0 \leq a \bmod n < n$.

We write $a \equiv b \pmod{n}$ if $(a \bmod n) = (b \bmod n)$, and say that $a$ is equivalent to $b$ modulo $n$. Equivalently, $a \equiv b \pmod{n}$ if and only if $n$ divides $b - a$.

\subsection{Polynomials}

\begin{definition}
A polynomial in $n$ of degree $d$ is a function:
$$p(n) = \sum_{i=0}^d a_i n^i$$
where the constants $a_0, a_1, \ldots, a_d$ are the coefficients and $a_d \neq 0$.
\end{definition}

A polynomial is asymptotically positive if and only if $a_d > 0$. For an asymptotically positive polynomial $p(n)$ of degree $d$, we have $p(n) = \Theta(n^d)$.

We say that a function $f(n)$ is \textbf{polynomially bounded} if $f(n) = O(n^k)$ for some constant $k$.

\subsection{Exponentials}

For all real $a > 0$, $m$, and $n$:
\begin{align*}
a^0 &= 1 \\
a^1 &= a \\
a^{-1} &= 1/a \\
(a^m)^n &= a^{mn} = (a^n)^m \\
a^m a^n &= a^{m+n}
\end{align*}

For all $n$ and $a \geq 1$, the function $a^n$ is monotonically increasing in $n$.

\begin{theorem}
For all real constants $a$ and $b$ such that $a > 1$:
$$\lim_{n \to \infty} \frac{n^b}{a^n} = 0$$
Therefore: $n^b = o(a^n)$
\end{theorem}

Thus, any exponential function with a base strictly greater than 1 grows faster than any polynomial function.

Using $e = 2.71828\ldots$ (the base of the natural logarithm):
$$e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots = \sum_{i=0}^\infty \frac{x^i}{i!}$$

For all real $x$: $e^x \geq 1 + x$ (equality only when $x = 0$)

\subsection{Logarithms}

Standard notations:
\begin{align*}
\lg n &= \log_2 n \quad \text{(binary logarithm)} \\
\ln n &= \log_e n \quad \text{(natural logarithm)} \\
\lg^k n &= (\lg n)^k \quad \text{(exponentiation)} \\
\lg \lg n &= \lg(\lg n) \quad \text{(composition)}
\end{align*}

For all real $a > 0, b > 0, c > 0$, and $n$:
\begin{align*}
a &= b^{\log_b a} \\
\log_c(ab) &= \log_c a + \log_c b \\
\log_b a^n &= n \log_b a \\
\log_b a &= \frac{\log_c a}{\log_c b} \quad \text{(change of base)} \\
\log_b(1/a) &= -\log_b a \\
\log_b a &= \frac{1}{\log_a b} \\
a^{\log_b c} &= c^{\log_b a}
\end{align*}

We say that a function $f(n)$ is \textbf{polylogarithmically bounded} if $f(n) = O(\lg^k n)$ for some constant $k$.

\begin{theorem}
For any constant $a > 0$:
$$\lim_{n \to \infty} \frac{\lg^b n}{n^a} = 0$$
Therefore: $\lg^b n = o(n^a)$
\end{theorem}

Thus, any positive polynomial function grows faster than any polylogarithmic function.

\subsection{Factorials}

The factorial function is defined for integers $n \geq 0$ as:
$$n! = \begin{cases}
1 & \text{if } n = 0 \\
n \cdot (n-1)! & \text{if } n > 0
\end{cases}$$

A weak upper bound is $n! \leq n^n$.

\begin{theorem}[Stirling's Approximation]
$$n! = \sqrt{2\pi n} \left(\frac{n}{e}\right)^n \left(1 + \Theta\left(\frac{1}{n}\right)\right)$$
\end{theorem}

Consequences:
\begin{align*}
n! &= o(n^n) \\
n! &= \omega(2^n) \\
\lg(n!) &= \Theta(n \lg n)
\end{align*}

\subsection{Fibonacci Numbers}

The Fibonacci numbers are defined by:
\begin{align*}
F_0 &= 0 \\
F_1 &= 1 \\
F_i &= F_{i-1} + F_{i-2} \quad \text{for } i \geq 2
\end{align*}

The golden ratio $\phi = \frac{1+\sqrt{5}}{2} \approx 1.618$ and its conjugate $\hat{\phi} = \frac{1-\sqrt{5}}{2} \approx -0.618$ are the roots of $x^2 = x + 1$.

The $i$-th Fibonacci number is:
$$F_i = \frac{\phi^i - \hat{\phi}^i}{\sqrt{5}}$$

Since $|\hat{\phi}| < 1$, we have $F_i = \lfloor \phi^i/\sqrt{5} + 1/2 \rfloor$, showing that Fibonacci numbers grow exponentially.

\section{Summary}

This chapter introduced the fundamental tools for analyzing algorithm efficiency:

\begin{itemize}
\item \textbf{Asymptotic notations} ($\Theta, O, \Omega, o, \omega$) provide a framework for classifying functions according to their growth rates
\item \textbf{$\Theta$-notation} gives asymptotically tight bounds
\item \textbf{$O$-notation} gives asymptotic upper bounds
\item \textbf{$\Omega$-notation} gives asymptotic lower bounds
\item \textbf{Standard functions} (polynomials, exponentials, logarithms, factorials) form a hierarchy of growth rates
\item \textbf{Key relationships}: Logarithmic $<$ Polynomial $<$ Exponential $<$ Factorial
\end{itemize}

These tools enable precise characterization of algorithm performance and facilitate meaningful comparisons between alternative algorithmic approaches.

\end{document}
